#!/usr/bin/python
# Copyright 2011 Benoit Sigoure
#
# This library is free software: you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this library.  If not, see <http://www.gnu.org/licenses/>.

# All the timestamps and durations in the script are in milliseconds unless
# noted otherwise.

"""Parses useful stats out of logs from `strace -tttTs300 -p <pid>'"""

from __future__ import with_statement

__author__ = "Benoit 'tsuna' Sigoure"

import Queue
import base64
import httplib
import os
import re
import socket
import subprocess
import sys
import threading
import time
import uuid
from optparse import OptionParser

try:
  import json
except ImportError:
  json = None


# UNIX username of the processes we want to trace, by default.
DEFAULT_USER = "www-data"
GEARMAN_DEFAULT_USER = "gearman"

VALID_METHODS = set(["GET", "POST", "HEAD", "OPTIONS"])

# We track the following syscalls that affect file descriptors.
# We assume the file descriptor number is the 1st argument.
TRACK_FD_SYSCALLS = set(["read", "readv", "recvfrom", "recvmsg",
                         "write", "writev", "sendto", "sendfile",
                         "bind", "connect", "bind", "poll",
                        ])
# Same thing but for system calls that are only used for RPCs.
# For these, even if we didn't see the connect() because we started tracing
# after the connection was establish, we'll count them as interactions with
# backends.
# Invariant: this set must be a subset of TRACK_FD_SYSCALLS.
TRACK_FD_RPC_SYSCALLS = set(["recvfrom", "sendto"])
# Syscalls which we don't support right now because I haven't seen Apache use
# them yet, although they have the same importance as `poll'.
# Invariant: this set is exclusive of TRACK_FD_RPC_SYSCALLS.
UNSUPPORTED_FD_SYSCALLS = set(["select", "pselect",
                               "epoll_wait", "epoll_pwait"])
# Any syscall that isn't in this set won't be parsed (fast-path).
TRACK_SYSCALLS = (TRACK_FD_SYSCALLS | UNSUPPORTED_FD_SYSCALLS
                  | set(["accept", "close"]))
# Traces to upload are added to this queue for a separate thread to send them.
TRACEQ = None
# Fields to extract from a trace and to index separately in the `summary'
# ElasticSearch type.
TRACE_SUMMARY_FIELDS = ("method", "resource", "request_ts", "end_to_end",
                        "num_syscalls", "slowest_syscall", "apache_closed",
                        "prev_slowest", "prev_connect")


def popen(cmdline, **kwargs):
  return subprocess.Popen(cmdline, stdin=subprocess.PIPE,
                          stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                          **kwargs)


class strace(object):
  """Context manager and iterator to run strace."""

  def __init__(self, pid):
    try:
      os.kill(pid, 0)
    except OSError, e:
      print >>sys.stderr, "PID %r: %s" % (pid, e)
      sys.exit(1)
    self.pid = pid

  def __enter__(self):
    self.proc = popen(["strace", "-tttTs300", "-p", str(self.pid)])
    self.proc.stdin.close()
    self.proc.stdout.close()
    return self

  def __iter__(self):
    return self.proc.stderr

  def __exit__(self, type, value, tb):
    os.kill(self.proc.pid, 15)   # Please die.
    self.proc.stderr.close()
    if self.proc.poll() is None:
      time.sleep(1)
      os.kill(self.proc.pid, 9)  # FOAD!
      self.proc.wait()


def find_worker(options):
  proc = ["pgrep", "-u", options.user, options.process_name]
  if options.pgrep_args:
    proc.insert(1, "-f")
  proc = popen(proc)
  stdout, stderr = proc.communicate()
  retval = proc.wait()
  if retval == 1:
    if stderr:
      print >>sys.stderr, stderr,
    print >>sys.stderr, ("No process matching %r found running as %s"
                         % (options.process_name, options.user))
    sys.exit(1)
  elif retval:
    assert 0, "pgrep returned %d, stderr=%s" % (retval, stderr)
  stdout = stdout.splitlines()
  pid = int(stdout[-1])
  if pid == os.getpid():  # Don't trace ourselves, that's silly.
    stdout.pop()
    pid = int(stdout[-1])
  print "strace'ing PID %d" % pid
  return pid


def lsof_tcp(pid, deadline=5):
  """Finds the remote IPv4 peers of all the TCP connections of the given PID.

  This function is not atomic so it may be missing some very recently created
  connections or even report an invalid remote peer if the FD gets closed and
  re-connected to another peer during our run.

  This function can be expansive on certain older versions of the Linux kernel
  if the machine is connected to many peers.

  Args:
    pid: The PID of the process, assumed to be valid.
    deadline: maximum time in seconds to spend in this function.  Bail out
      with incomplete results if more than this many seconds have elapsed.
  Yields:
    Tuples (fd, ip, port) for each FD connected to a remote peer.
  """
  procfd = "/proc/%d/fd" % pid
  if not os.path.isdir(procfd):
    return

  deadline += time.time()
  sockets = {}  # Maps the inode number of the socket to the FD num.
  for fd in os.listdir(procfd):
    try:
      dst = os.readlink("%s/%s" % (procfd, fd))
    except OSError:
      continue  # race: FD was closed, just ignore it.
    if not dst.startswith("socket:["):
      continue
    dst = dst[8:-1]
    assert int(dst) > 0, "negative inode? %r" % (dst,)
    sockets[dst] = int(fd)

  # Reading this file is *very* expansive with certain versions of the Linux
  # kernel when it's connected to many peers.
  with open("/proc/net/tcp") as f:
    f.readline()  # Skip the headers.
    i = 0
    while True:
      line = f.readline()
      if not line:  # EOF
        break
      if i == 10:
        if time.time() >= deadline:
          break
        i = 0
      i += 1
      line = line.split()
      inode = line[9]
      fd = sockets.get(inode)
      if fd is None:
        continue
      del sockets[inode]
      try:
        ip, port = line[2].split(":")
      except ValueError:
        print repr(line)
        raise
      ip = int(ip, 16)
      assert ip < 1<<32, "Not IPv4? %r" % (line,)
      port = int(port, 16)
      ip = ("%d.%d.%d.%d" % (ip & 0xFF,
                             (ip & 0xFF <<  8) >>  8,
                             (ip & 0xFF << 16) >> 16,
                             (ip & 0xFF << 24) >> 24))
      yield (fd, (ip, port))
      if not sockets:
        break  # Optimization: nothing left to search, early out.


def get_pre_existing_connections(options):
  connected_fds = {}
  timestamp = time.time() * 1000
  for fd, peer in lsof_tcp(options.pid, deadline=options.lsof_deadline):
    connected_fds[fd] = (0, peer, 0, "(pre-existing connection)")
  print ("lsof'ed %d pre-existing TCP connections in %7.2fms"
         % (len(connected_fds), time.time() * 1000 - timestamp))
  return connected_fds


def find_gearmand_socket(connected_fds):
  """Looks for a socket connected to a gearman server."""
  for fd, (timestamp, (host, port), retval, call) in connected_fds.iteritems():
    if port == 7003:
      return fd
  return None


def parse_call(line):
  """Parses a line returned by strace.

  Args:
    line: A line read from strace's output.
  Returns:
    A tuple (timestamp, syscall, call, retval, duration) where:
      - timestamp: is the time at which the call was made (in ms).
      - syscall: is the name of the syscall (e.g. "close").
      - call: is the call itself (e.g. "close(42)").
      - retval: is the integer return value.
      - duration: is the time the syscall took (in ms).
    If the syscall of this line isn't in TRACK_SYSCALLS, then the
    timestamp and the retval in the tuple returned will be None.
  """
  try:
    space = 18  # Initial timestamp has a fixed size, "1299473528.482648"
    if line[space - 1] != " ":
      raise ValueError("char %d isn't a space: %r" % (space - 1, line))
    paren = line.index("(", space)
    syscall = line[space:paren]
    lt = line.rindex("<", 0, -5) + 1
    duration = float(line[lt:-2]) * 1000  # s -> ms
    equal = line.rindex(" = ", 0, -9)
    call = line[space:equal]  # `line' but w/o timestamp / retval.
    # If we don't really care about this system call and we spent very little
    # time in it, let's not parse the timestamp and the return value.  This
    # micro-optimization saves up to 30% of the time we spend in this function,
    # which itself accounts for 25-35% of the runtime (so we save 3-9% total).
    # Yes, extracting a substring and converting it to a float takes that long.
    if syscall not in TRACK_SYSCALLS and duration < 1:
      return None, syscall, call, None, duration

    timestamp = float(line[:space - 1]) * 1000  # s -> ms
    equal += 3
    retval = int(line[equal:line.index(" ", equal)], 0)
  except ValueError, e:
    raise ValueError("%s in %r" % (e, line))
  return timestamp, syscall, call, retval, duration


def parse_http_request(line):
  # Parse the request out.
  resource = i = None
  try:
    request = line[line.index('"', 24)+1:line.rindex('"', 0, -10)]
    method = request[:request.index(" ", 3)]
    if method not in VALID_METHODS:
      print >>sys.stderr, "Couldn't parse the HTTP request in", repr(line)
      return "(invalid request)", "(unknown method)", "(unknown resource)"
    # The resource has to be on the 1st line, so up to the 1st \r\n
    resource = request[len(method)+1:request.find(r"\r\n", len(method) + 1)]
    # If we have a query string, remove it.
    i = resource.find("?", len(method) + 1)
    if i < 0:  # If there's no query string, remove at least the " HTTP/1.x"
      i = resource.rfind(" ")
    if i > 0:
      resource = resource[:i]
  except ValueError:
    print >>sys.stderr, ("parse_http_request line=%r, resource=%r, i=%r"
                         % (line, resource, i))
    raise
  return request, method, resource


def parse_connect(line):
  """Extracts the remote address out of a connect() syscall."""
  try:
    i = line.index("{", 26) + 1
    line = line[i:line.index("}", i)]
  except ValueError:
    print >>sys.stderr, "line=", repr(line)
    raise
  if line.startswith("sa_family=AF_INET"):
    i = line.index("sin_port=htons(") + 15
    port = int(line[i:line.index(")", i + 1)])
    i = line.index("sin_addr=inet_addr(\"") + 20
    host = line[i:line.index('")', i + 1)]
    return host, port
  return line, 0


def pretty_peer((host, port)):
  if port <= 0:
    return host
  fqdn = socket.getfqdn(host)
  if fqdn != host:
    return "%s %s:%d" % (fqdn, host, port)
  return "%s:%d" % (host, port)


def pretty_peers(peers):
  return ", ".join(pretty_peer(peer) for peer in peers)


def pretty_syscall(call, retval):
  """Pretty prints (shortens) a system call and its return value."""
  if retval is None:
    retval = "???"
  else:
    retval = str(retval)
  if len(call) > 92:
    return "%s... = %s" % (call[:92], retval)
  else:
    return "%s = %s" % (call, retval)


def parse_fd(call, syscall):
  i = 1
  if syscall == "poll":  # poll([{fd=117, events=...}], ...)
    i += 5  # Skip over "[{fd="
  return int(call[i + len(syscall):call.index(",", i)])


def straceprof(input, options):
  client_fd = None   # File descriptor number of the client socket.
  closed_cli = None  # Have we closed the connection to our client?
  request = None     # Raw HTTP request (possibly truncated).
  method = None      # HTTP method of the request.
  resource = None    # HTTP resource requested
  first_read_ts = 0  # Timestamp of the 1st read from client_fd.
  cli_read_ts = 0    # Timestamp at which we read the request from client_fd.
  first_write_ts = 0 # Time at which we wrote the response to the client_fd.
  cli_write_ts = 0   # Last timestamp at which we wrote to client_fd.
  num_syscalls = 0   # Number of system calls we did for this request.
  num_cli_read = 0   # Number of time we had to read from client_fd.
  num_cli_write = 0  # Number of time we had to write to client_fd.
  read_time = 0      # Total time needed to fully read the request.
  write_time = 0     # Total time needed to fully write the response.
  req_size = 0       # How many bytes we read from client_fd.
  resp_size = 0      # How many bytes we wrote to client_fd.
  syscalls_times = {}# Maps a syscall name to cumulative time spent in it.
  syscalls_count = {}# Maps a syscall name to the number of calls to it.
  slowest_syscall_time = 0
  slowest_syscall_name = None
  slowest_syscall_retv = None
  slowest_syscall_line = None
  connect_calls = 0  # Number of times we called connect() during this request.
  connected_to = set() # Set of backends we connected to during this request.
  prev_slowest = None  # (duration, name, retv, line) of the system call
                       # that preceded the slowest syscall.
  slowest_fd = None    # If known, FD of the slowest syscall.
  prev_connect = None  # Same thing but for the connect() on the FD of
                       # prev_slowest if prev_slowest is about a FD we track.
  known_fds = {}  # Maps a FD to a tuple (duration, name, retval, line)
                  # for the most recent call on that FD.
  connected_fds = None# Maps a FD to a tuple (timestamp, peer, retval, line)
                      # for when this FD was connected, if we saw the connect.
  backend_time = {}   # Maps a peer we interacted with during the request
                      # to the amount of time we spent interacting with it.
  backend_reqs = {}   # Maps a peer we interacted with during the request
                      # to a list of tuples (duration, peer, retval, line, ts).
  calls = []  # All the calls for this request.

  def print_report():
    """Prints a detailed report about the current query."""
    print method + " " + resource
    print ("  Request  size was %5d bytes%s."
           % (req_size, "" if read_time < 0.1 else
              " (read  the request  in %d reads  in %7.2fms)"
              % (num_cli_read, read_time)))
    print ("  Response size was %5d bytes%s."
           % (resp_size, "" if read_time < 0.1 else
              " (wrote the response in %d writes in %7.2fms)"
              % (num_cli_write, write_time)))
    print "  End-to-end processing time: %13.2fms" % end_to_end
    print ("    Time executing system calls: %10.2fms in %4d calls"
           % (sum(syscalls_times.itervalues()), num_syscalls))
    sorted_times = sorted(syscalls_times.iteritems(),
                          reverse=True,
                          key=lambda (syscall, duration): duration)
    for cumul_syscall, cumul_time in sorted_times[:options.top]:
      print ("      Including %4d %-7s calls: %7.2fms"
             % (syscalls_count[cumul_syscall], cumul_syscall, cumul_time))
    sum_backend_time = sum(backend_time.itervalues())
    if backend_time and sum_backend_time > 0.1:
      print ("      Interacting with %2d backends: %7.2fms"
             % (len(backend_time), sum(backend_time.itervalues())))
      if options.show_backend_times:
        for peer, timing in sorted(backend_time.iteritems(), reverse=True,
                                   key=lambda (peer, timing): timing):
          if peer[1] <= 0 and timing < 0.5:  # Not a TCP socket.
            continue  # Ignore insignificant AF_NETLINK / AF_FILE stuff.
          print ("%34s: %7.2fms" % (pretty_peer(peer), timing))
          if options.show_backend_reqs:
            for call in backend_reqs[peer]:
              print ("      %28s: %7.2fms %s %s"
                     % (call[1], call[0], " " if call[4] < cli_write_ts else "#",
                        pretty_syscall(call[3], call[2])))
    if connect_calls and options.show_connect:
      print ("      Including %4d connect calls to: %s"
             % (connect_calls, pretty_peers(connected_to)))
    print ("    Slowest system call: %-9s %8.2fms\n      %s"
           % (slowest_syscall_name, slowest_syscall_time,
              pretty_syscall(slowest_syscall_line, slowest_syscall_retv)))
    if prev_slowest is not None:  # => the slowest syscall was on an FD.
      print ("    Previous system call on this FD:\n      "
             + pretty_syscall(prev_slowest[3], prev_slowest[2]))
      if slowest_fd == client_fd:
        print "    This FD is the one connected to the client."
      elif prev_connect is not None:
        if prev_connect[0] > 0:
          print ("    This FD got connected %6.2fms ago to %s"
                 % (timestamp - prev_connect[0], pretty_peer(prev_connect[1])))
        else:
          print ("    This FD was already connected to %s"
                 % (pretty_peer(prev_connect[1])))
    if closed_cli:
      print "  Apache closed the connection to the client."

    if options.print_all:
      print "--- all syscalls for this request ---"
      for call in calls:
        print call,
      print "--- end syscalls for this request ---"

  def print_json_report():
    """Prints a detailed report about the current query, in JSON."""
    data = {
      "method": method, "request": request, "resource": resource,
      "request_ts": first_read_ts,
      "req_size": req_size, "resp_size": resp_size,
      "num_cli_read": num_cli_read, "read_time": read_time,
      "num_cli_write": num_cli_write, "write_time": write_time,
      "end_to_end": end_to_end,
      "num_syscalls": num_syscalls,
      "slowest_syscall": jsonify_syscall(slowest_syscall_time or 0,
                                         slowest_syscall_name,
                                         slowest_syscall_retv or -4242,
                                         slowest_syscall_line),
      "apache_closed": closed_cli,
    }
    timings = []
    for syscall, timing in syscalls_times.iteritems():
      timings.append({"name": syscall, "time": timing,
                      "count": syscalls_count[syscall]})
    data["syscalls_times"] = timings
    if prev_slowest is not None:
      data["prev_slowest"] = jsonify_syscall(*prev_slowest)
    be_reqs = []
    for (host, ip), be_calls in backend_reqs.iteritems():
      transformed_calls = []
      for call in be_calls:
        call = list(call)
        # Whether or not the syscall was in the fast path.
        call[4] = True if call[4] < cli_write_ts else False
        transformed_calls.append(jsonify_syscall(*call))
      be_reqs.append({"peer": pretty_peer((host, ip)),
                      "calls": transformed_calls})
      data["backend_reqs"] = be_reqs
    if slowest_fd == client_fd:
      data["prev_connect"] = {"client": True}
    elif prev_connect is not None:
      data["prev_connect"] = jsonify_connect(*prev_connect)

    if options.print_all:
      data["syscalls"] = calls

    if options.server is not None:
      try:
        TRACEQ.put_nowait(data)
      except Queue.Full:
        print >>sys.stderr, ("Trace queue is full, discarding trace: "
                             "%s %s %7.2fms" % (method, resource, end_to_end))
    else:
      print json_encode(data, options)

  connected_fds = getattr(options, "connected_fds", {})
  if connected_fds:
    del options.connected_fds
    if options.gearman:
      client_fd = find_gearmand_socket(connected_fds)

  for line in input:
    self_timing = time.time() * 1000
    # expected format for line:
    #  1299473528.478145 syscall(arg1, arg2) = 8 <0.000009>
    if not line:
      continue
    if options.print_all:
      calls.append(line)
    try:
      timestamp, syscall, call, retval, duration = parse_call(line)
    except ValueError:
      continue

    if syscall == "accept":
      client_fd = retval
    elif (options.gearman and syscall == "connect"
          and parse_connect(line)[1] == 7003):
      client_fd = parse_fd(call, syscall)
    elif client_fd is None:
      continue  # We don't know who we're talking to, skip until we do.
    else:
      if syscall in TRACK_FD_SYSCALLS:
        fd = parse_fd(call, syscall)
      else:
        fd = None
      # Don't count these for accept() itself.  We can hang in accept() for a
      # while if this worker doesn't get any work to do.
      num_syscalls += 1
      syscalls_times[syscall] = syscalls_times.get(syscall, 0) + duration
      syscalls_count[syscall] = syscalls_count.get(syscall, 0) + 1
      # Measure the slowest system call that's in the serving path.
      if request and duration > slowest_syscall_time:
        slowest_syscall_time = duration
        slowest_syscall_name = syscall
        slowest_syscall_retv = retval
        slowest_syscall_line = call
        if syscall in TRACK_FD_SYSCALLS:
          slowest_fd = fd
          prev_slowest = known_fds.get(fd)
          prev_connect = connected_fds.get(fd)
        else:
          slowest_fd = prev_slowest = prev_connect = None

    if timestamp is None:
      continue

    # If we successfully read something from the client socket...
    if (retval > 0
        and syscall == "read"
        and call.startswith("read(%d," % client_fd)):

      if cli_write_ts:
        end_to_end = cli_write_ts - cli_read_ts

      if (cli_read_ts and cli_write_ts
          and (options.show_options or method != "OPTIONS")
          and (not options.grep or options.grep.search(resource))
          and (not options.min_lat or end_to_end >= options.min_lat)):
        read_time += cli_read_ts - first_read_ts
        write_time += cli_write_ts - first_write_ts
        if options.json:
          print_json_report()
        else:
          print_report()
        options.nreqs += 1
        if options.max_reqs and options.nreqs >= options.max_reqs:
          return

      if cli_write_ts:
        closed_cli = False
        request = None
        num_syscalls = slowest_syscall_time = connect_calls = 0
        cli_read_ts = cli_write_ts = first_write_ts = 0
        num_cli_read = num_cli_write = read_time = write_time = 0
        connected_to.clear()
        syscalls_times.clear()
        syscalls_count.clear()
        backend_reqs.clear()
        backend_time.clear()
        req_size = resp_size = 0

      if request is None:  # First read we're doing for this request?
        if calls:
          calls = [calls[-1]]  # Keep the read.
        first_read_ts = timestamp
        if options.gearman:
          request = resource = call
          method = "(gearman)"
        else:
          request, method, resource = parse_http_request(line)
      # else: It took us more than one read to get the full request.
      cli_read_ts = timestamp
      num_cli_read += 1
      read_time += duration
      req_size += retval

    # If we write to the client socket, we're responding to the query.
    if (client_fd is not None
        and ((syscall == "writev"  # It seems Apache only uses writev.
              and call.startswith("writev(%d," % client_fd))
             or (syscall == "write"
                 and call.startswith("write(%d" % client_fd)))):
      if not first_write_ts:
        first_write_ts = timestamp
      cli_write_ts = timestamp
      num_cli_write += 1
      write_time += duration
      resp_size += retval

    # A FD was closed, forget what we track about it.
    elif retval == 0 and syscall == "close":
      fd = int(line[24:line.index(")", 25)])
      if fd == client_fd:
        closed_cli = True
        assert first_write_ts or request == "(invalid request)", \
          "FD to client closed without writing to it " + line
        client_fd = None
      else:
        if fd in known_fds:
          del known_fds[fd]
        if fd in connected_fds:
          peer = connected_fds[fd][1]
          del connected_fds[fd]
          if options.show_backend_reqs:
            if peer not in backend_reqs:
              backend_reqs[peer] = []
            backend_reqs[peer].append((duration, syscall, retval, call,
                                       timestamp))

    # Remember the last read / write on a FD.
    elif syscall in TRACK_FD_SYSCALLS:
      known_fds[fd] = (duration, syscall, retval, call)
      peer = None
      if (syscall == "connect"  # Extract the remote address
          or (syscall == "bind" and "sa_family=AF_NETLINK" in call)):
        peer = parse_connect(line)
        connected_fds[fd] = (timestamp, peer, retval, call)
        connected_to.add(peer)
        if peer not in backend_time:
          backend_time[peer] = 0
        connect_calls += 1
      elif fd in connected_fds:
        peer = connected_fds[fd][1]
        backend_time[peer] = backend_time.get(peer, 0) + duration
      elif syscall in TRACK_FD_RPC_SYSCALLS:
        # We use the negative value of the FD as the port number so that each
        # FD will be counted as a different peer.
        peer = (guess_host_type(syscall, call), -fd)
        connected_fds[fd] = (0, peer, 0, "(pre-existing connection)")
        backend_time[peer] = backend_time.get(peer, 0) + duration
      if peer and options.show_backend_reqs:
        if peer not in backend_reqs:
          backend_reqs[peer] = []
        backend_reqs[peer].append((duration, syscall, retval, call,
                                   timestamp))

    elif syscall in UNSUPPORTED_FD_SYSCALLS:
      print "  Warning: unsupported system call: %r" % (line,)

    self_timing = time.time() * 1000 - self_timing
    if self_timing > 10:
      print >>sys.stderr, ("warning: took %.2fms to process line %r"
                           % (self_timing, line))


def guess_host_type(syscall, call):
  if syscall == "sendto":
    if r'"version\r\n"' in call:  # Memcache clients frequently ask this.
      return "(unknown-memcache)"
    i = call.index(",", len(syscall))
    if call.startswith((', "get ', ', "set ', ', "add ', ', "delete '), i):
      return "(unknown-memcache)"
  return "(uknown-host)"


def jsonify_syscall(duration, name, retv, call, ts=None):
  result = {
    "duration": duration,
    "name": name,
    "retv": retv,
    "call": call,
  }
  if ts:
    result["timestamp"] = ts
  return result


PORT2SERVICE = {
  25:    "smtp",
  53:    "dns",
  80:    "http",
  443:   "https",
  3306:  "mysql",
  6379:  "redis",
  7003:  "gearmand",
  9090:  "thrift_hbase",
  9200:  "esearch",
}

def get_host_type(host, port):
  if port < 0:
    return host.strip("()").replace("-", "_")
  port = PORT2SERVICE.get(port, "port_%s" % port)
  if host.startswith("10."):
    return port
  elif host.startswith("127."):
    return "localhost_%s" % port
  else:  # Connection out to the intertubes.
    return "external_%s" % port


def getfqdn(host):
  # On OSX, for whatever reason, calling socket.getfqdn with "(foo)" in
  # argument locks up the Python interpreter pretty hard.  It gets stuck
  # in kevent, called by _mdns_query_mDNSResponder.  Really odd.
  if host and host[0] == "(":
    return host
  return socket.getfqdn(host)


def jsonify_connect(ts, (host, port), retv, call):
  fqdn = getfqdn(host)
  return {
    "peer": "%s:%d" % (host, port),
    "host": fqdn,
    "type": get_host_type(host, port),
    "retv": retv,
    "call": call,
    "timestamp": ts,
  }


def json_encode(data, options=None):
  if options is not None and options.json_indent is not None:
    return json.dumps(data, indent=options.json_indent)
  else:
    return json.dumps(data)


def gen_trace_id():
  return base64.urlsafe_b64encode(uuid.uuid4().bytes).rstrip("=")


def upload_thread(server, options):
  """Starts a separate thread to upload traces to the given server."""
  batch = []  # Items to send to ES.
  nitems = 0  # How many items are we sending to ES.
  batch_bytes = 0  # Total size of the items.
  # Actions to use to index data through ElasticSearch's /_bulk interface.
  index_summary = {"index": {"_index": options.index, "_type": "summary", "_id": None}}
  index_trace = {"index": {"_index": options.index, "_type": "trace", "_id": None}}
  index_fulltrace = {"index": {"_index": options.index, "_type": "fulltrace", "_id": None}}
  width = len(str(options.server_batch))
  while True:
    trace = TRACEQ.get()

    if trace is not TRACEQ:
      nitems += 1
      id = gen_trace_id()
      index_summary["index"]["_id"] = id
      batch.append(json_encode(index_summary))
      batch.append(json_encode(dict((key, trace.pop(key))
                                    for key in TRACE_SUMMARY_FIELDS
                                    if key in trace)))

      index_trace["index"]["_id"] = id
      batch.append(json_encode(index_trace))
      fulltrace = trace.pop("syscalls", None)
      trace = json_encode(trace)
      batch.append(trace)
      batch_bytes += len(trace)

      if fulltrace:
        index_fulltrace["index"]["_id"] = id
        batch.append(json_encode(index_fulltrace))
        batch.append(json_encode({"syscalls": fulltrace}))
        batch_bytes += len(batch[-1])

      if (nitems < options.server_batch
          and batch_bytes < options.server_size_threshold):
        continue
    elif not batch:  # trace is the poison pill and there's nothing batched
      break          # so quit immediately.

    # Flush the batch.
    nitems = len(batch) / 2  # Divide by 2 to not count the actions.
    batch = "\n".join(trace for trace in batch) + "\n"
    timing = time.time()
    server.request("POST", "/_bulk", batch,
                   {"Content-Type": "application/json"})
    try:
      resp = server.getresponse()
    except socket.error, e:
      print >>sys.stderr, "Failed to POST to ElasticSearch: %s" % e
      break
    if resp.status != httplib.OK:
      raise RuntimeError("Failed to upload traces to ElasticSearch: %s %s\n%s"
                         % (resp.status, resp.reason, resp.read()))
    resp = resp.read()
    timing = (time.time() - timing) * 1000
    resp = json.loads(resp)
    items = resp.get("items")
    if not items:
      raise RuntimeError("Empty response from ElasticSearch: %r" % (resp,))
    elif len(items) != nitems:
      raise RuntimeError("Unexpected number of results from ElasticSearch:"
                         " expected %d, got %d, response=%r"
                         % (nitems, len(items), resp))
    for item in items:
      item = item.get("index")
      if not item or not item.get("ok") or not item.get("_id"):
        raise RuntimeError("Weird response from ElasticSearch: %r" % (resp,))
    trace_items = [item["index"] for item in items
                   if item["index"]["_type"] == "trace"]
    print ("Created %*d traces (%5dKB) in %7.2fms, IDs: %s"
           % (width, len(trace_items), len(batch) / 1024, timing,
              " ".join(item["_id"] for item in trace_items)))
    batch = []
    nitems = batch_bytes = 0

    if trace is TRACEQ:  # If the queue was added in itself, it's a special
      break              # message ("poison pill") to tell us to exit.


def getopt(argv):
  parser = parser = OptionParser(description=__doc__)
  parser.add_option("-f", "--find-worker", dest="find_worker",
                    default=False, action="store_true",
                    help="Find a random worker to strace.")
  parser.add_option("-g", "--grep", dest="grep", default=None,
                    metavar="RE", help="Only report requests for HTTP"
                    " resources that match this regexp.")
  parser.add_option("-j", "--json", dest="json", default=False,
                    action="store_true",
                    help="Print output in JSON (default: %default).")
  parser.add_option("--json-indent", dest="json_indent", default=None,
                    type="int", help="Number of spaces to use to indent JSON"
                    " output (default: off).")
  parser.add_option("-l", "--min-latency", dest="min_lat", default=None,
                    type="int", metavar="MSEC", help="Only report request for"
                    " which end-to-end latency was higher than this.")
  parser.add_option("-m", "--max-requests", dest="max_reqs", default=None,
                    type="int", metavar="NUM", help="Stop after this many requests.")
  parser.add_option("-p", "--pid", dest="pid", default=None,
                    type="int", metavar="PID", help="Run strace on this pid.")
  parser.add_option("--pgrep-args", dest="pgrep_args",
                    default=False, action="store_true",
                    help="When using pgrep, also match process arguments"
                    " (default: %default).")
  parser.add_option("--process-name", dest="process_name", default="apache2",
                    metavar="RE", help="For use with -f/--find-worker, what"
                    " regexp to use with pgrep (default: %default).")
  parser.add_option("-s", "--server", dest="server", default=None,
                    metavar="HOST:PORT", help="Droopy server to send the traces"
                    " to, for storage in ElasticSearch (implies -j)"
                    " (default: off).")
  parser.add_option("-I", "--index", dest="index", default="droopy",
                    metavar="NAME", help="ElasticSearch index name to use"
                    " (default: %default).")
  parser.add_option("--server-timeout", dest="server_timeout", default=3.0,
                    metavar="SEC", type="float", help="Timeout when talking to"
                    " the --server (default: %default).")
  parser.add_option("--server-batch", dest="server_batch", default=25,
                    type="int", metavar="NUM", help="How many traces to send"
                    " at once (default: %default).")
  parser.add_option("--server-queue-size", dest="server_queue_size", default=None,
                    type="int", metavar="NUM", help="Maximum number of traces to"
                    " queue internally (default: 10 * --server-batch).")
  parser.add_option("--server-size-threshold", dest="server_size_threshold",
                    default=4096, type="int", metavar="KBYTES",
                    help="Traces bigger than this always trigger a flush"
                    " (default: %default).")
  parser.add_option("-t", "--top", dest="top", default=1,
                    type="int", metavar="NUM",
                    help="Print the top NUM system calls (default: %default).")
  parser.add_option("-u", "--apache-user", dest="user", default=None,
                    metavar="USER", help="Username under which Apache workers"
                    " run (default: %s)." % DEFAULT_USER)
  parser.add_option("-B", "--show-backend-time", dest="show_backend_times",
                    default=False, action="store_true",
                    help="Show time spent interacting with each backend"
                    " (default: %default).")
  parser.add_option("-C", "--show-connect", dest="show_connect",
                    default=False, action="store_true",
                    help="Show connect() system calls (default: %default).")
  parser.add_option("-G", "--gearman", dest="gearman",
                    default=False, action="store_true",
                    help="Slice requests of gearman worker (default: %default).")
  parser.add_option("-L", "--lsof-deadline", dest="lsof_deadline", default=3,
                    type="float", metavar="SEC", help="Spend only up to this"
                    " many seconds finding pre-existing TCP connections"
                    " (default: %default).")
  parser.add_option("-O", "--show-options-requests", dest="show_options",
                    default=False, action="store_true",
                    help="Show OPTIONS requests (default: %default).")
  parser.add_option("-P", "--print-all-syscalls", dest="print_all",
                    default=False, action="store_true",
                    help="Print all the calls for each requests"
                    " (default: %default).")
  parser.add_option("-R", "--show-backend-requests", dest="show_backend_reqs",
                    default=False, action="store_true",
                    help="Show requests to backends (implies -B)"
                    " (default: %default).")
  options, argv = parser.parse_args(argv)
  if options.grep:
    options.grep = re.compile(options.grep)
  if options.min_lat is not None and options.min_lat <= 0:
    parser.error("--min-latency must have a positive value")
  if options.max_reqs is not None and options.max_reqs <= 0:
    parser.error("--max-requests must have a positive value")
  options.nreqs = 0
  if options.server_batch <= 0:
    parser.error("--server-batch must be strictly positive")
  elif options.server_queue_size is None:
    options.server_queue_size = options.server_batch * 10
  elif options.server_queue_size <= 0:
    parser.error("--server-queue-size must be strictly positive")
  elif options.server_size_threshold <= 0:
    parser.error("--server-size-threshold must be strictly positive")
  if options.user is None:
    if options.gearman:
      options.user = GEARMAN_DEFAULT_USER
    else:
      options.user = DEFAULT_USER
  if options.pid is not None:
    if options.pid <= 1:
      parser.error("--pid must have a value strictly greater than 1")
    elif options.find_worker:
      parser.error("--find-worker and --pid are mutually exclusive")
  elif options.find_worker:
    options.pid = find_worker(options)
  if options.show_backend_reqs:
    options.show_backend_times = True
  if options.server is not None:
    options.server_size_threshold <<= 10  # Convert KB -> bytes.
    if options.server_timeout <= 0:
      parser.error("--server-timeout must be positive")
    if not options.index:
      parser.error("--index is required with --server")
    try:
      host, port = options.server.split(":")
    except ValueError:
      parser.error("--server must be of the form host:port")
    try:
      port = int(port)
      if port <= 0:
        raise ValueError()
    except ValueError:
      parser.error("--server port number %r is not a strictly positive integer"
                   % (port,))
    socket.setdefaulttimeout(options.server_timeout)
    server = httplib.HTTPConnection(host, port)
    try:
      server.connect()
    except socket.error, e:
      parser.error("Couldn't connect to %s:%d: %s" % (host, port, e))
    try:
      server.request("GET", "/_cluster/health")
      resp = server.getresponse()
      if resp.status != httplib.OK:
        parser.error("Server at %s:%d seems unhealthy and responded %s %s"
                     % (host, port, resp.status, resp.reason))
      resp.read()  # Needed to keep the connection alive.
    except (socket.error, httplib.HTTPException), e:
      parser.error("Couldn't health-check %s:%d: (%s) %s"
                   % (host, port, type(e).__name__, e))
    global TRACEQ
    TRACEQ = Queue.Queue(maxsize=options.server_queue_size)
    options.server = threading.Thread(name="upload", target=upload_thread,
                                      args=(server, options))
    options.server.start()
    options.json = True
  if options.json:
    if json is None:
      parser.error("JSON output requested but the json module isn't available")
    elif options.json_indent is not None and options.json_indent <= 0:
      parser.error("--json-indent must have a strictly positive value")
  return options, argv


def main(argv):
  #timing = time.time() * 1000
  options, argv = getopt(argv)
  try:
    for path in argv[1:]:
      if path == "-":
        straceprof(sys.stdin, options)
      else:
        with open(path) as f:
          straceprof(f, options)

    if options.pid is not None:
      # We need to do this before we start tracing because it can easily take
      # several seconds and we don't wanna block strace, otherwise we'll also
      # block the process we're tracing.  We put the result in `options' which
      # is really an ugly kludge.
      options.connected_fds = get_pre_existing_connections(options)
      with strace(options.pid) as f:
        straceprof(f, options)
  except KeyboardInterrupt:
    return 130
  finally:
    #timing = time.time() * 1000 - timing
    #print >>sys.stderr, "Self-reported timing: %.2fms" % timing
    if options.server is not None and options.server.is_alive():
      TRACEQ.put(TRACEQ)  # Poison pill to ask the thread to exit.
      options.server.join(options.server_timeout * 5)  # Allow a bit of time...
      if options.server.is_alive():
        print >>sys.stderr, "Uploader thread isn't exiting :(", options.server


if __name__ == "__main__":
  sys.exit(main(sys.argv))
